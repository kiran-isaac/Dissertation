\chapter{Cycle 1 --- Proof of Concept}
The goal of Cycle 1, which spanned approximately the first month of my project, was to arrive at a proof of concept. This cycle started at the beginning of the project with a discussion my supervisor, and the identification of my client. I proceeded by analysing the project requirements using autoethnographic methods, designing the system, designing \ac{SFL} and the Explorer, and then implementing the proof of concept. I then evaluated the merits and drawbacks of the proof of concept by speaking to my client. 

\section{Requirements Analysis}
\subsection{Autoethnography}
\label{sec:c1_autoethnography}
\begin{quote}
`Autoethnography is an ethnographic method in which the fieldworker's experience is investigated together with the experience of other observed social actors. \cite{autoethnography}'
\end{quote}

\noindent In this cycle, I took an autoethnographic approach to requirements analysis and to design. As the `fieldworker', I drew on my own experience being involved in teaching Haskell for the last two academic years. This experience was very valuable to this project, and it allowed me take the initial brief from my supervisor and effectively design a solution, and then quickly implement a proof of concept of this solution. 

\subsection{The Brief}
This project was proposed by my supervisor, Jess Foster. In our initial meeting, we discussed how she wanted a tool that would help build intuition for how functional languages are evaluated, that she could use to supplement her explanation of otherwise difficult to intuit functional language concepts. We also discussed the benefits of the tool being accessible to students to use themselves during labs or at home. Jess helped me to identify an appropriate client: Samantha Frohlich. Jess and Samantha are both lecturers on \hyperref[COMS10016]{COMS10016}. It was necessary to identify a client other than Jess, as her existing role as my supervisor/primary marker could limit guidance she would be able to give me if she were also my client. 

Following this meeting, I broke down this brief into smaller parts. Taking an autoethnographic approach, I used my own experience teaching functional languages to consider solutions and come up with requirements for each part. 

\subsubsection{Building Intuition}
\label{building_intuition}
This is the key to an effective solution. \sam{MOST students OF}Many students to the first year \ac{FP} \sam{fo here} unit do not have any experience with functional programming \sam{functional programming here},\sam{.} \sam{I disagree with the rest, cos we strive for the lectures to be engaging (so maybe you can say that the teaching style is already to make learning not passive, so this will further that. Also your comment about not attending or engaging with labs is irrelevant cos if they dont engage with labs why would they use your tool? }and find it difficult to gain an intuition from passively watching lectures, and do not attend or engage with labs. 

During cycle 2 \sam{but this is the cycle 1 section?} [REFHERE: Testathon], to confirm my belief that students found functional languages harder to learn, and harder to build an intuition about, I performed a survey. There were 15 participants, who were a mixture of undergraduate and postgraduate computer scientists, all of whom had taken the first year \ac{FP} unit. In one section of the survey, they were presented with a series of statements designed to gauge their feelings towards functional and imperative languages. A Likert scale\cite{likert1932technique} was used to measure the attitudes of participants towards the statements. See \ref{fig:imp_is_easy} for imperative results, and \ref{fig:fp_is_hard} for functional results. 

\begin{figure*}[ht]
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/imperative_likert.png}
        \caption{}
        \label{fig:imp_is_easy}
    \end{subfigure}
    
    \begin{subfigure}{\textwidth}
        \centering
        \includegraphics[width=\linewidth]{images/fp_likert.png}
        \caption{}
        \label{fig:fp_is_hard}
    \end{subfigure}
    \caption{The results of a survey performed during the cycle 2 [REFHERE: Testathon], where a Likert scale was used to gauge 15 participants feelings towards imperative (a) and functional (b) programming languages}
\end{figure*}

\sam{flip sentences so it is clear which language we are talking about BEFORE giving the number}
\sam{For imperative languages,} $80\%$ of respondents agreed/strongly agreed that they can program in an imperative language, similarly $80\%$ of respondents agreed/strongly agreed that they had an intuitive understanding of them. \sam{for functional languages,}$66.7\%$ of respondents strongly/agreed that they knew functional languages, but only $47.6\%$ of respondents would agree/strongly agree that they had an intuitive understanding of them.
\sam{no new line}
The number of people who claim to know how to program in functional languages is less than imperative, but more striking is the difference in reported `intuition'. 

% All participants who did not disagree that functional languages are hard to learn were presented with a free form text box, and asked to describe why. These responses were turned into a word cloud \ref{fig:fp_wordcloud}. Half of responses included the word `different'. 

In my experience \sam{teaching fp}, a very effective way to build intuition for functional programming languages is to demonstrate evaluation step by step. I frequently wrote out evaluations on paper for student during the \hyperref[COMS10016]{COMS10016} labs. I would also ask students to complete sections themselves. Others have also found that encouraging stepwise evaluation on paper is an effective way to get `a feeling for what a program does'. \cite{fp_first_year} [TODO: theres more things to cite here]

\sam{Thus} A tool to perform these step by step evaluations in an interactive manner would be very valuable. The tool should have an interface that allows progress to be made step by step, showing the history of past steps as well as giving information about the step about to be taken. This would allow students to understand and interact with a stepwise evaluation, without anyone having to undertake the long process of writing it out, and without risk of incorrectness. Furthermore, the effects of changing the input program could be seen quickly, providing instant feedback. 

\subsubsection{Use as a Lecture Tool} The tool should be suitable for use in lectures. It should provide an interface that facilitates quality explanation of functional programming languages. The interface must be understandable, for both \ac{FP} \sam{and here cos its the first ac use its fully expanded,  but you have already said FP and then functuional programming} `experts' (lecturers, advanced users) as well as people who have never seen a functional language before. The tool must also be portable, and not require a complex installation process. 

\subsubsection{Use as a Self Teaching Tool} The tool should be `self-explanatory' enough for people to use it on their own without expert help. It should be fairly intuitive, and should have all the information required to use it presented to users. The tool must also be portable, and not require a complex installation process. The less complex this tool is to use, the more people will use it. 

\subsubsection{Demonstrating \ac{FP} languages} The tool must contain, at its core, a \sam{it doesnt have to be haskell, but we will pick haskell to match the first year unit at bristol to allow for better evaluation}Haskell-like functional programming language in order to demonstrate Haskell. It could be included/required on the host machine, however creating a demonstration tool around Haskell would be difficult due to the sheer size of the language, the number of features, and the complexity of the type system. It would be better to create Haskell-like language with a strictly limited size and maximum clarity, and include this in the system. The programming language should be designed with simplicity and clarity at its heart.

\subsection{SFL Explorer}
The requirements extracted from autoethnographic methods, as well as from my initial supervisor meeting, came together to form the idea for SFL Explorer. 

The system should be a website to maximize portability. The system should include a functional programming language, as well as some sort of UI that allows a functional program in the language to be entered, and evaluated step by step in a visual manner. 

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=1\linewidth]{images/fp_is_hard_wordcloud.png}
%     \caption{A word cloud of answers entered into the free form text box presented to the 10 respondents who reported that they did not disagree that functional languages were hard to learn}
%     \label{fig:fp_wordcloud}
% \end{figure}

\subsubsection{The Simple Functional Language}
\label{design:goals}
The language was given a name reflecting its core design principle: Simplicity. More precisely, the programming language should be designed with the following design principles in mind:
\begin{enumerate}
    \item \textbf{It should be simple and easy to understand}. This requires that the language should not have features that users might find difficult to understand why they work. This means that the language should have very few inbuilt functions, all of which should be easy to understand why they work. 
    \item \textbf{It should be similar to existing functional languages}. This would allow users to be able to transfer their intuition to other languages. It should be similar in syntax (it should have similar tokens and structures), as well as semantics (it should work similarly). 
    \item \textbf{It should be powerful enough to explain key concepts.}
\end{enumerate}
The features that should be selected for the language are the features that maximize these goals for the minimum implementation complexity. Out of our design goals, 2 and 3 have the potential to be in conflict, as more expressive power often requires more complex syntax. We must ensure a sensible compromise between all of our goals, while accounting for implementation complexity. When adding features for the language, we must prioritize the features that allow explanation of the `core' features of functional languages, and de-prioritise features that are not so `core' to the understanding of functional languages. 

\subsubsection{The Explorer}
The website (the explorer) should include a code editor for people to enter programs. Including the functionality for the language inside the website rather than requiring complex client/server communication would simplify the system, as well as improving responsiveness. 

[TODO: Finish this bit, summarize requirements]

\section{Language Design}
In this section, I will discuss the design of \ac{SFL} with respect to the requirements. This is iteration 1 of the design, and it was the proof of concept.  

\subsection{Definitions}

\begin{syntax}[Lowercase and Uppercase ID syntax as regular expressions]
\label{def:identifier_syntax}
(Lowercase Identifier): \(id ::= [a..z][a..zA..Z0..9\_]*\)\newline
(Operator): \(op ::= + \mid - \mid \times \mid / \mid > \mid \ge \mid < \mid \le \mid== \mid \mathrel{\mathtt{!=}} \)\newline
(Uppercase Identifier): \(Id ::= [A..Z][a..zA..Z0..9\_]*\)
\end{syntax}
\sam{im so sorry but this looks ugly. I think it just needs more space, it is very crampt, this is a minor thing and can wait till the very end}

\subsection{Basic Syntax}
Lambda calculus is the basis of modern functional programming languages. As discussed in the background, lambda calculus consists of 3 structures: identifiers, application, and abstraction. \sam{sadly this effectly says in prose the same as the BG, the only thing the BG adds is the formal definition, which not everyone will be able to read} One common extra structure that functional languages implement is an assignment. This is where we label an identifier with a certain meaning, such that all references to the assignment henceforth are identical to a reference to the meaning assigned. For instance:
\begin{lstlisting}[]
f = (\x.x)
main = f y
\end{lstlisting}
Is identical to \sam{dont do this, cos it gives latex the chance to mess up your formatting. Always give your listings a name and use the name in a full sentence e.g. "For instance, listing 1 and listing 2 are semantically equivalent"}
\begin{lstlisting}[]
main = (\x.x) y
\end{lstlisting}
Note the use of \verb|"\"| instead of \(\lambda\) as it is the closest character available on most keyboards. A program is then defined as a set of assignments, and we pick one specific label name to mark the `entry-point' expression in the program. Haskell, as well as many other languages, uses `main' to represent a programs' entry point, so we may use main. 

We must \sam{why must? (because we want to run our code), this is also a common extension of the STLC} also add a way to represent values, such as integers and booleans, to our language. Most programming languages, including functional ones, at least support integers. Booleans are also often supported to represent the results of integer comparison. Without literal values, programs would have to use complicated encodings (such as church numerals) to represent these values, making programs look more complicated.  \sam{bring this why forward}
\sam{this is a very typical para for you: i encourage you to structure your paras as because of x we need y, instead of we need x because of y}
\sam{no new line}
These two features massively shorten and simplify programming in this language.

\sam{this is not referenced anywhere. Above you say we need x and then just dump x here. Instead motivate, introduce and explain. What additions have been made to execute x. Explain your definitions like you would explain your code}
\begin{syntax}[The basic syntax of \ac{SFL}]
(Expression) \(E, F ::= [-][0, 1, ..]\mid E\; op\; F \mid true \mid false \mid id \mid \setminus id. E \mid E\:F\)\newline
(Assignment) \(A ::= id = E\)\newline
(Module) \(M ::= A\: M \mid End\)
\end{syntax}

\subsection{Reduction and Progress}
As discussed in the background, functional programs progress via reduction. \ac{SFL}  programs can reduce when we have an abstraction applied to a term. 

We may also want to replace variables with their assigned values. This is not reduction, however it is still progress

\todo{FINISH, will be easier when the bg on reduction is improved}

\section{Implementation}
\subsection{The Abstract Syntax Tree}
The tree structure of \ac{SFL} requires the following different types of tree nodes:
\begin{itemize}
    \item Identifier
    \item Literal
    \item Pair
    \item Application
    \item Abstraction
    \item Match
    \item Assignment
    \item Module
\end{itemize}
\sam{bullets look silly}

Initially, the approach taken when implementing this tree structure was to have each node `owning' its child nodes (see \ref{bg:rust}). However, it will be necessary frequently to be able to find nodes based on certain conditions (for example, the condition that this node is a valid redex) and then provide a value that represents the location of this node within the tree. Even if each of the tree nodes had a unique ID, locating a node from this value representing its location will require some sort of tree search.

Rather than this solution, which would have a non-constant node lookup time, a secondary structure can be used to store the tree nodes with constant time lookup, and then each node can store a value enabling constant time lookup of its children within this structure. In the implementation, these types were labelled as AST and ASTNode, where AST was an array of ASTNodes, and each ASTNode stored their children's indices in this array. The position in the array of an ASTNode will be referred to as its index.

\begin{figure}[t]
    \centering
    \begin{tabular}{|c|}
    \hline
    \begin{lstlisting}[language=Rust]
struct AST {
    vec: Vec<ASTNode>,
    root: usize,
}

enum ASTNodeType {
    Identifier,
    Literal,
    Pair,
    Application,
    Assignment,
    Abstraction,
    Module,
} 

struct ASTNodeSyntaxInfo {...}

struct ASTNode {
    t: ASTNodeType,
    token: Option<Token>,
    children: Vec<usize>,
    line: usize,
    col: usize,
    type_assignment: Option<Type>,
    additional_syntax_information: ASTNodeSyntaxInfo
}
    \end{lstlisting}
    \\\hline
    \end{tabular}
    \caption{The Rust code listing for the definition of the AST, with lifetime specifiers, accessibility modifiers, and the syntax information (see \ref{paragraph:to_string}) removed for conciseness.}
    \label{fig:ast_lst}
\end{figure}

See \ref{fig:ast_lst} for the code listing for the AST definition. In this implementation, \verb|Vec| was used for the array, as it is growable, resizeable, and facilitates constant-time lookup of its elements. The AST stores and owns all of the nodes, as well as storing the index of the root node rather than requiring it to be at a specific index. 

The node indices in the \verb|children| vector represent different things depending on what kind of node it is. 
\begin{itemize}
    \item If it is an abstraction, the first node represents the variable (or pair of variables) abstracted over, and the second node represents the expression.
    \item If it is an application, the first node is the function, and the second is the argument.
    \item If it is a pair, the first node is the first in the pair, and the second is the second in the pair.
    \item If it is a match expression, the first node represents the matched value, then after this it consists of the case followed by the resulting expression. Match expressions will always therefore have an odd number of children.
    \item If it is a module, then each of the children is an assignment
    \item If it is an assignment, then the first child is the variable being assigned to, and the second is the expression.
\end{itemize}

\verb|Literal| and \verb|Identifier| nodes store the tokens that defined them, so the strings can be accessed. \verb|Identifier| nodes used as abstraction arguments. These types can either be specified in the source program, or inferred later. Nodes also store their positions (line and column) in the source program, which can be used for error messages. 

In order to effectively explain the structure of a parsed program going forwards, the following structure will be used to give a written representation of an AST:
\begin{itemize}
    \item Nodes are represented as one line each, where, with the name of the node type, followed by its value for \verb|Literal|s and \verb|Identifier|s.
    \item The children of a node are all of the nodes with an indentation level one deeper than the node in question listed directly below it, until a shallower or equal depth node is listed. 
\end{itemize}

\noindent
For instance, 
\begin{lstlisting}
main = (\x.1) 2
\end{lstlisting}
would be represented as:
\begin{lstlisting}
Module:
  Assignment:
    Identifier: main
    Application:
      Abstraction:
        Identifier: x
        Literal: 1
      Literal: 2
\end{lstlisting}

\subsubsection{With the Benefit of Hindsight} % for jess's benefit as she did not like my AST definition so i want to aknowledge what she said about it
This project was my first major project using Rust. Below is a discussion of some Rust features which were not fully taken advantage of in this definition of syntax trees, followed by a discussion of the combination of these features that would have been more optimal. 

\paragraph{Tagged Unions}
An alternative implementation could have involved \verb|ASTNodeType| being a tagged union, with different node types being associated with different children and data items. For instance, application could be represented by \verb|Application(f: usize, x: usize)|, and identifiers could be 

\noindent\verb|Identifier(String)|. This would be more space efficient, as each node requires different data. It would also more elegantly represent the fact that each type of node is a different thing, and de-obfuscate the meaning of each of the different fields of a node. 

\paragraph{References}
This definition of the AST and the nodes has a parent object owning all of the nodes. As previously discussed, this was done to enable constant-time lookup of nodes from their indices. However, all things in a program already have such a reference enabling constant time lookup: a pointer, represented in rust by a reference. This was not used, as there were concerns about ensuring validity of each reference, and avoiding use-after-free bugs. These concerns were unfounded, as one of Rust's major features is that it provides safety guarantees ensuring that these problems are never encountered \cite{rust_book}. An object can only store a reference to another object if it can be guaranteed that it exists, and it will continue to exist for at least as long as the object storing the reference will. This is achieved via lifetime checking, using either inferred or explicitly stated specifiers of how long the two objects will exist relative to each other. 

\paragraph{A Better Implementation}
\ref{fig:ast_lst_2} shows an implementation that uses tagged unions to store information that is different for different node types, and pointers to the nodes directly rather than list indices. This avoids the possibility of referencing nodes that don't exist. It is also easier to understand what is common between nodes (syntax info) and what is uncommon. It is also more space efficient as it only stores the information that each type requires. The size of the improved implementation is 88 bytes, and the size of the original implementation is 128 bytes. The improved implementation is subjectively more elegant and readable. Objectively, it also takes up less space. It also forces memory safety, without the need for carefully implemented getter and setter functions. 

Despite this, the decision was made not to update the implementation for several reasons. The AST is so central to the implementation, that it would take a long time to switch properly. Memory and speed are not major constraints for this project, but implementation time is. Furthermore, as long as all indices used are either produced by a helper function, or the AST root, there should not be a problem with memory safety. 

\begin{figure}[t]
    \centering
    \begin{tabular}{|c|}
        \hline
    \begin{lstlisting}[language=Rust]
struct AST<'a> {
    vec: Vec<ASTNode<'a>>,
    root: &'a ASTNode<'a>,
}

enum ASTNodeType<'a> {
    Identifier{name: String},
    Literal{value: String, _type: PrimitiveType},
    Pair{first: &'a ASTNode<'a>, second: &'a ASTNode<'a>},
    Assignment{to: String, expr: &'a ASTNode<'a>, type_assign: Type},
    Abstraction{var: String, expr: &'a ASTNode<'a>, type_assign: Type},
    Module{assigns: Vec<&'a ASTNode<'a>>},
    Match{expr: &'a ASTNode<'a>, cases: Vec<&'a ASTNode<'a>>}
} 

struct ASTNodeSyntaxInfo { ... }

struct ASTNode<'a> {
    t: ASTNodeType<'a>,
    info: ASTNodeSyntaxInfo
}
    \end{lstlisting}
    \\\hline
    \end{tabular}
    \caption{An alternative implementation with a few advantages over the actual implementation. }
    \label{fig:ast_lst_2}
\end{figure}

\subsection{Methods on the AST}
Below are a selection of the more important or interesting methods implemented on the AST and its nodes.

\paragraph{Adding new nodes} We will frequently want to add new nodes to the tree. Where they are inserted is not important, so the tree will add them to the end, and return their index. These methods are needed extensively for the parser.

\paragraph{Getting children of nodes} As the interpretation of the \verb|children| array for each node changes depending on what type of node it is, a series of getters are implemented, such as `\verb|get_func|' to get the function of an application. These methods are needed extensively for the type checker, and the redex finding system. 

\paragraph{Substitute variable} Substitutes all instances of a variable in an expression with a given expression. This is needed for applying abstractions. For instance, the process of reducing \verb|(\x.x) 1|, is:
\begin{itemize}
    \item Get the name of the variable abstracted over: \verb|x|.
    \item Replace all instances of x in the abstraction expression with the right hand side of the application: \verb|1|.
    \item Replace all references to the abstraction with references to the abstractions expression. 
\end{itemize}

Note that this orphans the node for the abstraction, and the node for the abstraction variable \verb|x|. This is hard to rectify as deleting any nodes will shift the whole list, which would invalidate indies of nodes, which will break many of the references. This is rectified by cloning the AST, as described below.

\paragraph{Clone} The AST, or just a subsection of the AST from a given node, can be cloned by starting from the desired new root, and cloning each nodes children recursively. The new indices of each node may not be the same, as they may be moved in the list, but they will all be in the same place relative to each other. This also removes orphaned nodes, as they will never be cloned as they have no parents. 

\paragraph{To String} \label{paragraph:to_string} Programs can also be effectively transformed back into strings. This requires a few other pieces of information to be associated with some tree nodes, to make the output program as similar to the input program as possible. The more similar the output is to the input, the easier it is to understand. Some examples include:
\begin{itemize}
    \item Whether the application was generated by using the right associative \verb|$| operator in order to avoid parenthesis, for instance \verb|id $ 1 + 1|. [TODO: fix the way that the dollar sign operator works. It can be redefined as an inbuilt as its just an infix op]
    \item Whether the assignment, where the expression is an abstraction, was generated using the syntax \verb|x = \a.e| or the syntax \verb|x a = e|. 
\end{itemize}
We must also take into account whether a binary infix operator was used to generate a function call, and if so we must place it in the middle of its arguments. 

% There is also other syntax sugar, such as turning a List from `Cons' syntax into more familiar braced syntax, with comma separation.   For instance: \verb|Cons 1 (Cons 2 (Cons 3 Nil))| should be displayed as \verb|[1,2,3]|. [TODO: Consider whether to have only literals in this syntax or everything. Maybe toggleable syntax sugar?]

\begin{figure}[t]
    \centering
    \begin{tabular}{|c|}
        \hline
    \begin{lstlisting}[language=Rust]
enum DiffElem {
    Similarity(String),
    Difference(String, String)
}

type Diff = Vec<DiffElem>

function diff(ast1, ast2, expr1, expr2) -> Diff {
    node1, node2 = ast1.get(expr1), ast2.get(expr2)
    diff = new Diff;
    match (node1, node2) {
        case (ID, ID)
        case (Lit, Lit) {
            if node1.value == node2.value {
                diff += Similarity(node1.value)
            } else {
                diff += Difference(node1.value, node2.value)
            }
        }

        case (Pair {first1, second1}, Pair {first2, second2}) {
            diff += Similarity("(")
            diff += diff(ast1, ast2, first1, first2)
            diff += Similarity(",")
            diff += diff(ast1, ast2, second1, second2)
            diff ++ Similarity(")")
        }

        ...
    }
    
    return diff;
}
    \end{lstlisting}
    \\\hline
    \end{tabular}
    \caption{The Rust code listing for the type of the output of the AST::diff function, as well as a small section of the algorithm. There is also (not shown) a wrapper around the Diff type, to allow for conversion into JavaScript (see \ref{bg:wasm}), as well as the some logic for combining diffs.}
    \label{fig:diff_list}
\end{figure}

\paragraph{Diff} \label{paragraph:diff} Our frontend requires the ability to see what has changed between two program states. Highlighting these changes make understanding the changes in the users program in the frontend easier. This function generates the strings for the two trees simultaneously, producing the similarities and differences. \ref{fig:diff_list} shows a subsection of this algorithm, showing how it works for IDs, Literals and Pairs.

\subsection{The Parser}
The parser needs to consume a program, and return the following things:
\begin{itemize}
    \item The AST
    \item The `Label Table': The types of all labels defined, including both those defined explicitly (assignments) or implicitly (constructors). This is implemented as a struct `\verb|LabelTable|' which is a wrapper around a \verb|HashMap<String, Type>| with some useful methods. 
    \item The `Type Table': All type constructors and concrete types defined, stored with their arities. This is implemented as: \verb|HashMap<String, usize>|.
\end{itemize}
For instance, from the program:
\begin{lstlisting}[]
data List a = Cons a (List a) | Nil
double x = x * 2	
main :: List Int
main = Cons (double 1) (Cons (double 2) Nil)
\end{lstlisting}
We should extract the following data:
\begin{itemize}
    \item The AST: 
    \begin{lstlisting}[]
Module:
  Assignent
    Identifier: double
    Abstraction
      Identifer: x
      App
        App
          Identifier: +
          Identifier: x
        Literal: 2
    \end{lstlisting}
    \item All the known type assignments (excluding inbuilts)
        \begin{itemize}
            \item Cons: \(\forall a. a \Rightarrow List\;a\Rightarrow List\;a\)
            \item Nil: \(\forall a. List\;a\)
            \item main: \(\forall a. List\;a\)
        \end{itemize}
    \item The names of all known types (excluding inbuilts) 
        \begin{itemize}
            \item List, with an arity of 1
        \end{itemize}
\end{itemize}

The parser will also store a set of all bound variables at each location. This will allow us to disqualify some invalid programs while generating the tree, rather than having to traverse it after generation to catch these issues. For instance, we must the following assignments:
\begin{itemize}
    \item \verb|x = (\x. e)| where e is a valid expression, as x is ambiguous during the expression e. This would be disqualified when attempting to parse the abstraction as x is already bound.  
    \item \verb|x = y| where y is undefined.
\end{itemize}

\subsubsection{Lexical Analysis}
Lexical analysis is the process splitting a program into its constituent tokens (Lexemes). For instance, the program \verb|main = (\x.x) 1| is the following stream of tokens: \[[Id: main, Assignment, LeftParen, Backslash, Id: x, Dot, Id: x, RightParen, Literal: 1]\]
See \ref{appx:tokens} for the code listing of the definition of the tokens output by the lexical analysis.

The lexer loads the entire string into memory at once. This is not typical, as this can lead to problems with large files. The approach discussed in \cite{dragon_book} relies on a system of two buffers only holding individual pages of the file from disk. However, this system will not be loading files from disk; the program string is already in memory as it comes from the UI. Therefore, there would be no benefit to a more traditional lexer optimised to reduce memory usage. 

The lexer provides a \verb|next_token| function that returns the next token, and advances the pointer to the start of the token after. The lexer keeps track of line and column information, which is stored in the token to then be stored in the AST. 

\subsubsection{Expression Parsing}
Expressions are parsed using recursive descent parsing. Some of the techniques used for this part of the parser were inspired by the discussion of top down parsing in \cite{dragon_book}. 

At the top level, the expression parsing method is \verb|parse_expression|. A variable \verb|left| stores what is currently the index of the expression. It is called \verb|left| as if we encounter a token that denotes that \verb|left| is applied to whatever comes next, it becomes the left hand side of the application. \verb|left| is originally set to be the output of parsing a primary (see \ref{impl:parsing_primary}), and then progresses differently based on the next token.  Below are some of the ways that \verb|parse_expression| could proceed.
\begin{itemize}
    \item If the next token is an open bracket, we consume the token and then parse an expression. We then expect a closing bracket. We set \verb|left| to the application of \verb|left| to the expression
    \item If the next token is a dollar sign, we consume the token and then parse an expression. We do not expect a closing bracket, and we error if we receive one. We set \verb|left| to the application of \verb|left| to the expression.    
    \item If the next token is a token denoting the start of a primary expression structure:
    \begin{itemize}
        \item A backslash, indicating the start of a lambda
        \item An identifier, indicating a variable.
        \item A literal
    \end{itemize}
    We parse a primary, and set \verb|left| to the application of \verb|left| to our primary.
    \item If the next token is:
    \begin{itemize}
        \item A closing bracket
        \item EOF
        \item A newline
        \item An opening brace (indicating the end of parsing the matched expression of a match statement)
        \item A double colon, indicating a type assignment follows
    \end{itemize}
    We return \verb|left|. 
\end{itemize}

\paragraph{Primary Parsing}
\label{impl:parsing_primary}
A primary is a less complex structure than an expression. In this system, a primary is any expression structure other than applications. The primaries are:
\begin{itemize}
    \item Literals
    \item Identifiers
    \item Lambdas
    \item Expressions in brackets
\end{itemize}
Each of these have their own specific parsing algorithms, which may include calling \verb|parse_expression|. 

\paragraph{Literal and Identifier Parsing}
Literals and identifiers are turned trivially into their respective AST Nodes. For instance, the token:
\begin{verbatim}
Token {
    line: 0,
    col: 0,
    tt: TokenType::IntLiteral
    value: "2"
}
\end{verbatim}
Is turned into this ASTNode:
\begin{verbatim}
ASTNode {
    t: ASTNodeType::Literal,
    token: Some({the token}),
    children: [],
    line: 0,
    col: 0,
    type_assignment: Option<Primitve::Int>,
    additional_syntax_information: ...
}
\end{verbatim}

\paragraph{Parsing Abstractions}
Abstractions (in the simple case) are parsed by:
\begin{itemize}
    \item Consuming a lambda (represented by `\verb|\|' for ease of typing on standard keyboards)
    \item Parsing a variable. This variable must be added to our set of `bound' variables.
    \item Consuming the dot separator`\verb|.|'
    \item Parsing an expression
    \item Constructing an abstraction node from the variable and the expression
\end{itemize}

However, the definition of abstractions has a few complicating elements of syntax sugar.

\subparagraph{Abstractions May be Assignments}
The assignment \verb|f x = x| is implicitly \verb|f = \x. x|. This is solved by parsing an argument to \verb|parse_abstraction| representing whether this is an assignment. If it is an assignment, we do not parse the lambda, and expect the assignment operator `\verb|=|' as our separator rather than the dot. As previously mentioned in\ref{paragraph:to_string}, in order to output the string in a format that is as close as possible to the input, we set a flag in the \verb|ASTSyntaxInfo|: \verb|assign_abst_syntax| to all abstraction nodes defined like this. 

\subparagraph{Abstractions May Have Multiple Variables}
The abstraction \verb|\x y. x| is syntax sugar for \newline\noindent\verb|\x. (\y. x)|. Additionally, with the assignment syntax, \verb|f x y = x| is syntax sugar for \newline\noindent\verb|f = \x. (\y. x)|. This can be accounted for by continually parsing variables until we encounter `\verb|.|' or the assignment operator `\verb|=|', and then producing a series of abstractions over these variables in order. 

To parse an identifier, we must also check that the identifier is bound at this location.

\subsection{Web UI \ac{MVP}}
As part of this section, I also developed the MVP for the Web UI. \ref{fig:screenshot_cycle_1_end} shows the web UI after this stage of the project. 

\begin{figure}[h]
    \centering
    \includegraphics[width=1\linewidth]{images/cycle-1-end.png} 
    \captionsetup{justification=centering}
    \caption{The Web UI \ac{MVP}, as presented to my client at the end of cycle 1.}
    \label{fig:screenshot_cycle_1_end}
\end{figure}

Until this point, development was done in one rust package. This package would be compiled to a binary and run natively, with a basic \ac{CLI}. This needed to be changed so that it can compile to \ac{WASM} and run in the web browser. As I wanted to keep the \ac{CLI} for debugging, as well as for use later, I did not want to change the whole project to a project with a \ac{WASM} interface. A solution to keeping both interfaces was to separate the functionality that would be common to the \ac{CLI} as well as the \ac{WASM} library into a separate library, and then have the two interfaces as separate packages that depended on this one. The structure of the project became the following 4 packages. 

\begin{itemize}
    \item \textbf{libsfl}: All of the language functionality, as this is common to both interfaces
    \item \textbf{sflcli}: All of the original CLI functionality without the language functionality.  
    \item \textbf{libsfl\_wasm}: a package set up for use with wasm-pack (\ref{bg:wasm-pack}). It provides a wrapper around \textbf{libsfl}, with wrapper functions returning data structures supported by wasm-bindgen\ref{bg:wasm-bindgen}. wasm-pack would compile this to an \ac{NPM} package containing:
    \begin{itemize}
        \item The WASM binary blob of the compiled rust code
        \item A JavaScript file that would load the blob into the browsers' memory, and provides methods that can call the appropriate the methods in the blob
        \item A TypeScript file providing the types of all of the packages exported functions. 
    \end{itemize}
    \item The Vite+React frontend (see \ref{bg:frontend}) that requires the package that results from compiling \textbf{libsfl\_wasm}
\end{itemize}


[TODO: More stuff about WASM bindings??]

\section{Proof of Concept Client Meeting: Evaluation and \newline Next Steps}
\label{eval:c1}
At the end of the cycle, I presented the proof of concept project to my client, who was very positive about the project and its potential. The discussion was informal, a friendly conversation rather than a structured interview, to allow the direction of questioning to change depending on the clients answers. The meeting started with me giving my client a demo of the proof of concept using by using the system to evaluate the following program:
\begin{verbatim}
fac n = if n <= 1 then 1 else n * (fac (n - 1))
main = fac 5
\end{verbatim}

Below is a summary of my clients thoughts about various aspects of the proof of concept system and potential future iterations

\subsection{Usefulness as a Teaching Tool}
Below are some notes on what the client thought about the effectiveness of the project as a teaching tool, and how it could be improved in future iterations.
\begin{itemize}
    \item The project is already very useful as a teaching tool to demonstrate:
    \begin{itemize}
        \item Evaluation order, and the importance of laziness
        \item Currying
        \item Recursion
        \item The \lcalc
    \end{itemize}

    \item On top of this my client wanted to be able to use the system to demonstrate: 
    \begin{itemize}
        \item High Priority
        \begin{itemize}
            \item List and common list functions such as `map' and `fold'. These do not have to by polymorphic, they could be just defined over $Int$s or some other type. These also do not need to be user-definable, they can be built in. 
            \item Pattern Matching
        \end{itemize}
        \item Lower Priority 
        \begin{itemize}
            \item User definable data types, preferably polymorphic. This would mean we could define `$List$' as part of the language which would be good for clarity. 
        \end{itemize}
    \end{itemize}
\end{itemize}

\subsection{The Existing Language}
\paragraph{Positives:}
\begin{itemize}
    \item The language looked similar to Haskell. Particularly, the \verb|if _ then _ else| syntax, and the function assignment shorthand syntax (\verb|fac n = ...| rather than \verb|fac = \n. ...|, even though these are identical)
    \item The language is minimal and clear
    \item The factorial function was quite elegant, and it would be understandable to people who did not know Haskell.  
\end{itemize}

\paragraph{Negatives:}
My client had no specific complaints about the language as it currently stands, however we agreed was lacking many important features. The most difficult things to teach are concepts involving more complex data types. 

\paragraph{Requested Features:}
Below are the specific features my client asked for in order for the system to be able to demonstrate the things she wants to use the system to demonstrate:
\begin{itemize}
    \item Recursive Types
    \item Polymorphism
    \item Type Aliases
    \item Typechecking
    \item User Definable Data Types
\end{itemize}

\subsection{The Existing UI/UX}
\paragraph{Positives:}
\begin{itemize}
    \item The editor, as it feels like a very popular editor: VSCode
\end{itemize}

\paragraph{Negatives}
\begin{itemize}
    \item It is unstable. This is bad in a teaching tool, as it would waste a lot of time if it constantly broke in the lecture. 
    \item `laziest' as an option is confusing, as it was unclear if it was referring to one of the other on screen options, or if it was referencing a `hidden' option
    \item The vertical bar separating redex from contraction on the progress buttons was not obvious enough. On top of this, the bar was not centred, so it was hard to look through all the redexes at once as they were not aligned with each other. 
\end{itemize}

\paragraph{Requested Features}
\begin{itemize}
    \item Syntax highlighting, to make the language easier to read   
    \item A history of what the expression has been is vital to demonstrate step by step evaluation. I identified this as an important feature at the beginning of the cycle (see \ref{building_intuition}), but I had not finished it by the client meeting. It was implemented in the next cycle (see \ref{c2_poc_ui_impl}). 
    \item Sample Programs
\end{itemize}

\subsection{Conclusion}
At the end of this cycle, and going into cycle 2, I had a strong proof of concept system and an idea for how the system will look. The meeting with my client yielded many ideas, all of which I sucessfully implemented throughout this project. 