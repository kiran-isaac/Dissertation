% The document class supplies options to control rendering of some standard
% features in the result.  The goal is for uniform style, so some attention 
% to detail is *vital* with all fields.  Each field (i.e., text inside the
% curly braces below, so the MEng text inside {MEng} for instance) should 
% take into account the following:
%
% - author name       should be formatted as "FirstName LastName"
%   (not "Initial LastName" for example),
% - supervisor name   should be formatted as "Title FirstName LastName"
%   (where Title is "Dr." or "Prof." for example),
% - degree programme  should be "BSc", "MEng", "MSci", "MSc" or "PhD",
% - dissertation title should be correctly capitalised (plus you can have
%   an optional sub-title if appropriate, or leave this field blank),
% - dissertation type should be formatted as one of the following:
%   * for the MEng degree programme either "enterprise" or "research" to
%     reflect the stream,
%   * for the MSc  degree programme "$X/Y/Z$" for a project deemed to be
%     X%, Y% and Z% of type I, II and III.
% - year              should be formatted as a 4-digit year of submission
%   (so 2014 rather than the academic year, say 2013/14 say).

\documentclass[
author={Kiran Sturt},
degree=BSc,
title={Implementing a Step by Step Evaluator for a Simple Functional Programming language},
unit={COMS30045},
oneside]{dissertation}

\usepackage[utf8]{inputenc}
\usepackage{amsthm}

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\itshape}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec

\theoremstyle{definition}
\theoremstyle{break}
\newtheorem{syntax}{Definition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% -----------------------------------------------------------------------------

% This macro creates the AI declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makeaidecl

% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------

\chapter*{Ethics Statement}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}
\label{chap:supporting_tech}

\noindent
This section should present a detailed summary, in bullet point form, 
of any third-party resources (e.g., hardware and software components) 
used during the project.  Use of such resources is always perfectly 
acceptable: the goal of this section is simply to be clear about how
and where they are used, so that a clear assessment of your work can
result.  The content can focus on the project topic itself (rather,
for example, than including ``I used \mbox{\LaTeX} to prepare my 
dissertation''); an example is as follows:

\begin{quote}
\noindent
\begin{itemize}
\item I used React (\url{https://react.dev/}) to develop the website for this project.
\item The bindings for the web assembly interface to the library for the languge were generated by using macros from the wasm-pack rust crate: \url{https://github.com/rustwasm/wasm-pack}.
\item I used GitHub Copilot to help assist with generating unit tests.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\chapter{Introduction}
\label{chap:context}

In this dissertation, I present SFL-explorer, a tool to demonstrate how a functional programming language is evaluated.

SFL-explorer takes the form of a functional language (SFL), packaged with some interfaces that allows users to observe the process of evaluation of a term as a series of step by step or multi-step reductions, and control the order that sub-terms are evaluated. Two interfaces are provided, a command line interface and a web application. The ultimate goal of this project was to make a tool that makes learning and teaching the basics of functional programming easier. There are two groups of people the project is designed to be of interest to:
\begin{itemize}
    \item Those involved in learning functional languages. These could be students of a university course, or anyone interested in the topic.
    \item Those involved in teaching functional languages, as part of a course or otherwise.
\end{itemize}

The language itself is not meant to be the main interest for the users of this system. It is designed to be fairly generic, with syntax and semantics similar to popular functional languages, so that users can take their understanding from using SFL-explorer and apply it to these languages. 

% -----------------------------------------------------------------------------

\chapter{Background}
\label{chap:technical}

\section{Rust}
This project is written in rust. Some of the decisions made, particularly in the implementation of the AST, require an understanding of Rust, especially the memory management model.

"Ownership" is an important concept. The rules of ownership \cite{rust_book}:
\begin{itemize}
    \item Each value in Rust has an owner.
    \item There can only be one owner at a time.
    \item When the owner goes out of scope, the value will be dropped.
\end{itemize}   

If a value is owned in one scope, but another scope needs to read/write it, we may use a reference to the value. The rules of references \cite{rust_book}:
\begin{itemize}
    \item At any given time, you can have either one mutable reference or any number of immutable references. 
    \item References must always be valid.
\end{itemize}

These rules ensure that immutable references are to things that don't change, and references are always to things that exist.

% -----------------------------------------------------------------------------

\chapter{Design}
\label{chap:design}

\section{Language Design}
SFL is designed with the following goals in mind:
\begin{enumerate}
    \item SFL should be similar to existing functional languages
    \item SFL should be simple and easy to understand. 
\end{enumerate}

\begin{syntax}[Lowercase and Uppercase ID syntax as regular expressions]
\label{def:identifier_syntax}
(Lowercase Identifier): \(id ::= [a..z][a..zA..Z0..9\_]*\)\newline
(Uppercase Identifier): \(Id ::= [A..Z][a..zA..Z0..9\_]*\)
\end{syntax}

\subsection{Basic Syntax}
Lambda calculus is the basis of modern functional programming languages. As discussed in the background, Lambda calculus consists of 3 structures: identifiers, application, and abstraction. One common extra structure that functional languages implement is an assignment. This is where we label an identifier with a certain meaning, such that all references to the assignment henceforth are identical to a reference to the meaning assigned. For instance:

\begin{lstlisting}[]
f = (\x.x)
main = f y
\end{lstlisting}

Is identical to

\begin{lstlisting}[]
main = (\x.x) y
\end{lstlisting}

Note the use of \verb|"\"| instead of \(\lambda\) as it is the closest character available on most keyboards. A program is then defined as a set of assignments, and we pick one specific label name to mark the "entry-point" expression in the program. Haskell, as well as many other languages, use "main" to represent a programs entry point, so we may use main. 

We must also add a way to represent values, such as integers and booleans, to our language. Most programming languages, including functional ones, at least support integers. Booleans are also often supported to represent the results of integer comparison. Without literal values, programs would have to use complicated encodings (such as church numerals) to represent these values, making programs look more complicated. 

These two features massively shorten and simplify programming in this language.

\begin{syntax}[The basic syntax of SFL]
(Expression) \(E ::= [-][0, 1, ..] \mid true \mid false \mid id \mid \setminus id. E \mid E\:F\)\newline
(Assignment) \(A ::= id = E\)\newline
(Module) \(M ::= A\: M \mid End\)
\end{syntax}

\subsection{Type System}
We must have types representing integers and booleans in our language, if we are to effectively check the validity of expression containing their respective literals. Many languages, including Haskell, also have Algebraic Data Types allowing us to "Compose" other data types. In Haskell, these consist of 

Type names, as well as constructor names, start with uppercase letters in Haskell. This allows them to be easily differentiated from type variables, as well as regular variables. 
In the below definition of the SFL type system, we define \(\Alpha\) as the set of valid type names starting with uppercase letters defined by \(Id\), and \(\alpha\) as the set of valid type variable names defined by \(id\)

\begin{syntax}[SFL Types]
(Inbuilts): \(B::=Int\mid Bool\)\newline
(Monotype): \(\tau, \sigma ::= \alpha \mid B \mid \tau \rightarrow \sigma \mid (\tau, \sigma) \mid \Alpha \;T, U,...\)\newline
(Alias): \(A ::= Id = T\)\newline
(Type): \(T, U ::= \alpha \mid B \mid T \rightarrow U \mid (T, U) \mid \forall a. T \mid \Alpha \;T, U,...\)

\end{syntax}

\subsection{Case}

\subsection{Syntax Sugar}

% -----------------------------------------------------------------------------

\chapter{Implementation}
\label{chap:execution}

\section{The Abstract Syntax Tree}
The tree structure of SFL requires the following different types of tree nodes:
\begin{itemize}
    \item Identifier
    \item Literal
    \item Pair
    \item Application
    \item Abstraction
    \item Case
    \item Assignment
    \item Module
\end{itemize}
Initially, the approach taken when implementing this tree structure was to have each node "owning" its child nodes (see ownership). However, it will be necessary frequently to be able to find nodes based on certain conditions (for example, the condition that this node is a valid redex) and then provide a value that represents the location of this node within the tree. Even if each of the tree nodes had a unique ID, locating a node from this value representing its location will require some sort of tree search.

Rather than this solution, which would have a non-constant node lookup time, a secondary structure can be used to store the tree nodes with constant time lookup, and then each node can store a value enabling constant time lookup of its children within this structure. In the implementation, these types were labelled as AST and ASTNode, where AST was an array of ASTNodes, and each ASTNode stored their children's indices in this array. The position in the array of an ASTNode will be referred to as its index.

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Rust]
struct AST {
    vec: Vec<ASTNode>,
    root: usize,
}

enum ASTNodeType {
    Identifier,
    Literal,
    Pair,
    Application,
    Assignment,
    Abstraction,
    Module,
} 

struct ASTNodeSyntaxInfo {...}

struct ASTNode {
    t: ASTNodeType,
    token: Option<Token>,
    children: Vec<usize>,
    line: usize,
    col: usize,
    type_assignment: Option<Type>,
    additional_syntax_information: ASTNodeSyntaxInfo
}
    \end{lstlisting}
    \end{tabular}
    \caption{The Rust code listing for the definition of the AST, with lifetime specifiers, accessibility modifiers, and the syntax information (see \ref{paragraph:to_string}) removed for conciseness.}
    \label{fig:ast_lst}
\end{figure}

See \ref{fig:ast_lst} for the code listing for the AST definition. In this implementation, \verb|Vec| was used for the array, as it is growable, resizeable, and facilitates constant-time lookup of its elements. The AST stores and owns all of the nodes, as well as storing the index of the root node rather than requiring it to be at a specific index. 

The node indices in the \verb|children| vector represent different things depending on what kind of node this is. 
\begin{itemize}
    \item If this is an abstraction, the first node represents the variable (or pair of variables) abstracted over, and the second node represents the expression.
    \item If this is an application, the first node is the function, and the second is the argument.
    \item If this is a pair, the first node is the first in the pair, and the second is the second in the pair.
    \item If this is a match expression, the first node represents the matched value, then after this it consists of the case followed by the resulting expression. Match expressions will always therefore have an odd number of children.
    \item If this is a module, then each of the children is an assignment
    \item If this is an assignment, then the first child is the variable being assigned to, and the second is the expression.
\end{itemize}

\verb|Literal| and \verb|Identifier| nodes store the tokens that defined them, so the strings can be accessed. \verb|Identifier| nodes used as abstraction arguments. These types can either be specified in the source program, or inferred later. Nodes also store their positions (line and column) in the source program, which can be used for error messages. 

In order to effectively explain the structure of a parsed program going forwards, the following structure will be used to give a written representation of an AST:
\begin{itemize}
    \item Nodes are represented as one line each, where, with the name of the node type, followed by its value for \verb|Literal|s and \verb|Identifier|s.
    \item The children of a node are all of the nodes with an indentation level one deeper than the node in question listed directly below it, until a shallower or equal depth node is listed. 
\end{itemize}
\filbreak\noindent
For instance, 
\begin{lstlisting}
main = (\x.1) 2
\end{lstlisting}
would be represented as:
\begin{lstlisting}
Module:
  Assignment:
    Identifier: x
    Application:
      Abstraction:
        Identifier: x
        Literal: 1
      Literal: 2
\end{lstlisting}


\subsection{Methods on the AST}
Below are a selection of the more important or interesting methods implemented on the AST and its nodes.

\paragraph{Adding new nodes} We will frequently want to add new nodes to the tree. Where they are inserted is not important, so the tree will add them to the end, and return their index. These methods are needed extensively for the parser.

\paragraph{Getting children of nodes} As the interpretation of the \verb|children| array for each node changes depending on what type of node it is, a series of getters are implemented, such as "\verb|get_func|" to get the function of an application. These methods are needed extensively for the type checker, and the redex finding system. 

\paragraph{Substitute variable} Substitutes all instances of a variable in an expression with a given expression. This is needed for applying abstractions. For instance, the process of reducing \verb|(\x.x) 1|, is:
\begin{itemize}
    \item Get the name of the variable abstracted over: \verb|x|.
    \item Replace all instances of x in the abstraction expression with the left hand side of the application: \verb|1|.
    \item Replace all references to the abstraction with references to the abstractions expression. 
\end{itemize}
Note that this orphans the node for the abstraction, and the node for the abstraction variable \verb|x|. This is hard to rectify as deleting these nodes will shift the rest of the list after it left, which will break many of the references. This is rectified by cloning the AST, as described below.

\paragraph{Clone} The AST, or just a subsection of the AST from a given node, can be cloned by starting from the desired new root, and cloning each nodes children recursively. The new indices of each node may not be the same, as they may be moved in the list, but they will all be in the same place relative to each other. This also removes orphaned nodes, as they will never be cloned as they have no parents. 

\paragraph{To String} \label{paragraph:to_string} Programs can also be effectively transformed back into strings. This requires a few other pieces of information to be associated with some tree nodes, to make the output program as similar to the input program as possible. The more similar the output is to the input, the easier it is to understand. Some examples include:
\begin{itemize}
    \item Whether the application was generated by using the right associative \verb|$| operator in order to avoid parenthesis, for instance \verb|id $ 1 + 1|.
    \item Whether the assignment, where the expression is an abstraction, was generated using the syntax \verb|x = \a.e| or the syntax \verb|x a = e|. These are functionally equivalent, but it is a useful distiction to maintain clarity
\end{itemize}
This is needed for the visualisation of the program state as it changes. 

\section{Types}
Rust allows us to represent our types, as defined in [REFHERE: Type system], quite easily using Enums. Rust's Enums are an example of algebraic data types, and are therefore very useful for defining our own algebraic data type system. See \ref{fig:type_lst} for the listing. 

\begin{figure}[t]
    \centering
    \begin{lstlisting}[language=Rust]
pub enum Primitive {
    Int64,
    Bool,
}

pub enum Type {
    Unit,
    Primitive(Primitive),
    Function(Box<Type>, Box<Type>),
    TypeVariable(String),
    Forall(String, Box<Type>),
    Product(Box<Type>, Box<Type>),
    Union(String, Vec<Type>),
    Alias(String, Box<Type>),

    Existential(usize),
}
    \end{lstlisting}
    \caption{The Rust code listing for the definition of types}
    \label{fig:type_lst}
\end{figure}

We must use \verb|Box<Type>|, which represents a pointer to a heap allocated object, otherwise it would be impossible to calculate the size of \verb|Type|, as it could be infinitely large with it containing another \verb|Type| recursively. \verb|Box<Type>| however has known size, which is the size of a pointer on system in question. We also define Existential, as an implementation detail needed for the type checker. 

\section{The Parser}
The parser needs to consume a program, and return the following things:
\begin{itemize}
    \item The program's AST
    \item All labels with known types, including both those defined explicitly (assignments) or implicitly (constructors for data types)
    \item All types defined, as a map from the type name to the type it represents
\end{itemize}
For instance, from the program:
\begin{lstlisting}[]
data List a = Cons a (List a) | Nil
double x = x * 2	
main :: List Int
main = Cons (double 1) (Cons (double 2) Nil)
\end{lstlisting}
We should extract the following data:
\begin{itemize}
    \item The AST: {\filbreak
    \begin{lstlisting}[]
Module:
  Assignent
    Identifier: double
    Abstraction
      Identifer: x
      App
        App
          Identifier: +
          Identifier: x
        Literal: 2
    \end{lstlisting}}
    \item All the known type assignments (excluding inbuilts)
        \begin{itemize}
            \item Cons: \(\forall a. a \Rightarrow List\;a\Rightarrow List\;a\)
            \item Nil: \(\forall a. List\;a\)
            \item main: \(\forall a. List\;a\)
        \end{itemize}
    \item The names of all known types (excluding inbuilts) 
        \begin{itemize}
            \item List: \(List\;a\)
        \end{itemize}
\end{itemize}

The parser will also store a set of all bound variables at each location. This will allow us to disqualify some invalid programs. For instance, we must disqualify:
\begin{itemize}
    \item \verb|x = (\x. e)| where e is any expression, as x is ambiguous during the expression e.
    \item \verb|main = x| as x is undefined.
\end{itemize}

\subsection{Lexical Analysis}
Lexical analysis is the process splitting a program into its constituent tokens. For instance, the program \verb|main = (\x.x) 1| is the following stream of tokens: \[[Id: main, Assignment, LeftParen, Backslash, Id: x, Dot, Id: x, RightParen, Literal: 1]\]
See \ref{appx:tokens} for the code listing of the definition of the tokens output by the lexical analysis.

\subsection{Expression Parsing}
Expressions are parsed using recursive descent parsing. Some of the techniques used for this part of the parser were inspired by the discussion of top down parsing in \cite{dragon_book}. 

\subsection{Type Assignment and Definition Parsing}
We must also be able to parse type assignments (\verb|x :: Int|) and type definitions (\verb|type a |. Type assignments consist of the label being assigned, followed by the type expression. 

\subsubsection{Parsing Type Expressions}

\subsubsection{Parsing Data Declaration} 
As discussed in [REFHERE: Language Design], we want to be able to define and parse \verb|data| declarations. A \verb|data| deceleration consists of: 
\begin{itemize}
    \item The \verb|data| keyword
    \item The name of the type (uppercase ID)
    \item The Assignment Operator (=)
    \item A set of constructors separated by \(\mid\). 
        \begin{itemize}
            \item The name of the constructor
            \item The type expression, representing what type the constructor can be applied to 
        \end{itemize}
\end{itemize}

\subsection{Module Parsing}
% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{Appendix A: AI Usage}
\label{appx:ai_prompt}

I did not directly prompt any Large Language Models, or any other AI model, to assist with the writing of my dissertation or implementation. However, as listed in the Supporting Technologies list, I used GitHub Copilot to help with writing some tests for the parser and type checker. I used it via the VS Code extension, which uses the context of your file, to provide advanced AI autocompletion.

% =============================================================================

\chapter{Appendix B: Tokens for Lexical Analysis}
\label{appx:tokens}
Below is the code for how tokens outputted by lexical analysis are defined. 
\begin{lstlisting}
enum TokenType {
    EOF,
    Newline,

    Id,
    UppercaseId,

    If,
    Then,
    Else,

    Match,
    LBrace,
    RBrace,

    IntLit,
    FloatLit,
    StringLit,
    CharLit,
    BoolLit,

    DoubleColon,
    RArrow,
    Forall,
    KWType,
    KWData,

    LParen,
    RParen,

    Lambda,

    Dollar,
    Dot,
    Comma,
    Bar,

    Assignment,
}

struct Token {
    tt: TokenType,
    value: String,
}
\end{lstlisting}

\end{document}
