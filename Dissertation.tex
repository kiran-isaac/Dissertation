\documentclass[twoside]{dissertation}

\usepackage[utf8]{inputenc}
\usepackage{amsthm}

\newtheoremstyle{break}% name
  {}%         Space above, empty = `usual value'
  {}%         Space below
  {\itshape}% Body font
  {}%         Indent amount (empty = no indent, \parindent = para indent)
  {\bfseries}% Thm head font
  {.}%        Punctuation after thm head
  {\newline}% Space after thm head: \newline = linebreak
  {}%         Thm head spec

\theoremstyle{definition}
\theoremstyle{break}
\newtheorem{syntax}{Definition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\begin{document}

% =============================================================================

% This macro creates the standard UoB title page by using information drawn
% from the document class (meaning it is vital you select the correct degree 
% title and so on).

\maketitle

% After the title page (which is a special case in that it is not numbered)
% comes the front matter or preliminaries; this macro signals the start of
% such content, meaning the pages are numbered with Roman numerals.

\frontmatter


%\lstlistoflistings

% The following sections are part of the front matter, but are not generated
% automatically by LaTeX; the use of \chapter* means they are not numbered.

% -----------------------------------------------------------------------------

\chapter*{Abstract}

% -----------------------------------------------------------------------------


\chapter*{Dedication and Acknowledgements}

% -----------------------------------------------------------------------------

% This macro creates the standard UoB declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makedecl

% -----------------------------------------------------------------------------

% This macro creates the AI declaration; on the printed hard-copy,
% this must be physically signed by the author in the space indicated.

\makeaidecl

% -----------------------------------------------------------------------------

% LaTeX automatically generates a table of contents, plus associated lists 
% of figures and tables.  These are all compulsory parts of the dissertation.

\tableofcontents
\listoffigures
\listoftables

% -----------------------------------------------------------------------------

\chapter*{Ethics Statement}

% -----------------------------------------------------------------------------

\chapter*{Supporting Technologies}
\label{chap:supporting_tech}

\begin{quote}
\noindent
\begin{itemize}
\item I used React (\url{https://react.dev/}) to develop the website for this project.
\item The bindings for the web assembly interface to the library for the language were generated by using macros from the wasm-pack rust crate: \url{https://github.com/rustwasm/wasm-pack}.
\item I used GitHub Copilot to help assist with generating unit tests.
\end{itemize}
\end{quote}

% -----------------------------------------------------------------------------

\chapter*{Notation and Acronyms}

% =============================================================================

% After the front matter comes a number of chapters; under each chapter,
% sections, subsections and even subsubsections are permissible.  The
% pages in this part are numbered with Arabic numerals.  Note that:
%
% - A reference point can be marked using \label{XXX}, and then later
%   referred to via \ref{XXX}; for example Chapter\ref{chap:context}.
% - The chapters are presented here in one file; this can become hard
%   to manage.  An alternative is to save the content in seprate files
%   the use \input{XXX} to import it, which acts like the #include
%   directive in C.

\mainmatter


\chapter{Introduction}
\label{chap:context}

In this dissertation, I present SFL-explorer, a tool to demonstrate how a functional programming language is evaluated.

SFL-explorer takes the form of a functional language (SFL), packaged with some interfaces that allows users to observe the process of evaluation of a term as a series of step by step or multi-step reductions, and control the order that sub-terms are evaluated. Two interfaces are provided, a command line interface and a web application. The ultimate goal of this project was to make a tool that makes learning and teaching the basics of functional programming easier. There are two groups of people the project is designed to be of interest to:
\begin{itemize}
    \item Those involved in learning functional languages. These could be students of a university course, or anyone interested in the topic. 
    \item Those involved in teaching functional languages, as part of a course or otherwise.
\end{itemize}

The language itself is not meant to be the main interest for the users of this system. It is designed to be fairly generic, with syntax and semantics similar to popular functional languages, so that users can take their understanding from using SFL-explorer and apply it to these languages. 

% -----------------------------------------------------------------------------

\chapter{Background}
\label{chap:technical}

\section{Rust}
\label{bg:rust}
This project is written in rust. Some of the decisions made, particularly in the implementation of the AST, require an understanding of Rust, especially the memory management model.

"Ownership" is an important concept. The rules of ownership \cite{rust_book}:
\begin{itemize}
    \item Each value in Rust has an owner.
    \item There can only be one owner at a time.
    \item When the owner goes out of scope, the value will be dropped.
\end{itemize}   

If a value is owned in one scope, but another scope needs to read/write it, we may use a reference to the value. The rules of references \cite{rust_book}:
\begin{itemize}
    \item At any given time, you can have either one mutable reference or any number of immutable references. 
    \item References must always be valid.
\end{itemize}

These rules ensure that immutable references are to things that don't change, and references are always to things that exist.

% -----------------------------------------------------------------------------

\chapter{Design}
\label{chap:design}

\section{Language Design}
\subsection{Goals}
SFL is designed with the following goals in mind:
\begin{enumerate}
    \item SFL should be similar to existing functional languages.
    \item SFL should be simple and easy to understand. 
    \item SFL should be powerful.
\end{enumerate}
The features that should be selected for SFL are the features that maximise these goals for the minimum implementation complexity. The language's syntax and type system should also work towards these goals. 

Out of our design goals, 2 and 3 have the potential to be in conflict, as more expressive power often requires more complex syntax. We must ensure a sensible compromise between all of our goals, while accounting for implementation complexity. 

When adding features for the language, we must prioritise the "core" features of functional languages, and de-prioritise features that are not so "core" to the understanding of functional languages. 

yadadada we should implement features that allow us to implement
\begin{itemize}
    \item Complex data structures, lists and trees
    \item Fold
    \item IO???
\end{itemize}

\subsection{Definitions}

\begin{syntax}[Lowercase and Uppercase ID syntax as regular expressions]
\label{def:identifier_syntax}
(Lowercase Identifier): \(id ::= [a..z][a..zA..Z0..9\_]*\)\newline
(Uppercase Identifier): \(Id ::= [A..Z][a..zA..Z0..9\_]*\)
\end{syntax}

\subsection{Basic Syntax}
Lambda calculus is the basis of modern functional programming languages. As discussed in the background, Lambda calculus consists of 3 structures: identifiers, application, and abstraction. One common extra structure that functional languages implement is an assignment. This is where we label an identifier with a certain meaning, such that all references to the assignment henceforth are identical to a reference to the meaning assigned. For instance:
\begin{lstlisting}[]
f = (\x.x)
main = f y
\end{lstlisting}
Is identical to
\begin{lstlisting}[]
main = (\x.x) y
\end{lstlisting}
Note the use of \verb|"\"| instead of \(\lambda\) as it is the closest character available on most keyboards. A program is then defined as a set of assignments, and we pick one specific label name to mark the "entry-point" expression in the program. Haskell, as well as many other languages, use "main" to represent a programs entry point, so we may use main. 

We must also add a way to represent values, such as integers and booleans, to our language. Most programming languages, including functional ones, at least support integers. Booleans are also often supported to represent the results of integer comparison. Without literal values, programs would have to use complicated encodings (such as church numerals) to represent these values, making programs look more complicated. 

These two features massively shorten and simplify programming in this language.

\begin{syntax}[The basic syntax of SFL]
(Expression) \(E ::= [-][0, 1, ..] \mid true \mid false \mid id \mid \setminus id. E \mid E\:F\)\newline
(Assignment) \(A ::= id = E\)\newline
(Module) \(M ::= A\: M \mid End\)
\end{syntax}

\subsection{Type System}
We must have types representing integers and booleans in our language, if we are to effectively check the validity of expression containing their respective literals. 

Many languages, including Haskell, also have Algebraic Data Types allowing us to "Compose" other data types. Algebraic data types are isomorphic to an algebraic expression consisting of sums and products of their constituent types. An example of a product type is the tuple \((Int,Bool)\) which is isomorphic to \(Int \times Bool\). Most languages have product types, which often take the form of structs or tuples. An example of a sum type is the Haskell syntax tagged union \verb!"data Shape = Circle Int | Rectangle Int Int"!, which is isomorphic to the type \(Int + (Int \times Int)\). 

We will now consider generic data structures, such as a lists that can hold any value of type $a$, written as \(List\;a\)". Here, \(List\) is not a type in itself, but it represents a constructor that takes a type, and returns a concrete type. We could write this as "$Type \rightarrow Type$", indicating that it behaves like a function, but at the type level rather than the value level. If we were to apply the constructor \(List\) to the concrete type \(Int\), the resulting type would be \(List \;Int\). 

Polymorphism as described here is first order polymorphism, as opposed to higher-order polymorphism (also known as higher-kinded polymorphism) where a type can abstract over a type that abstracts over a type \cite{pierce2002types}. An example of a function that is higher-order polymorphic is a function that takes a function, and then applies it to two differently typed values:
\begin{lstlisting}
applyToBoth :: (forall a. a -> a) -> a -> b -> (a, b)
applyToBoth f x y = (f x, f y)
\end{lstlisting}
This requires the ability to parse expressions with nested \verb|foralls|, as well as support during type inference for higher-kinded types. We do not think that this is a priority for the system, as 

These first-order polymorphic type constructors would be useful to have in SFL, with one example of their utility being defining the polymorphic function "\verb|length :: List a -> Int|" which should work regardless of what type the list is over. Higher order polymorphism is less important

The "Type of a Type" is known as its \emph{kind} \cite{pierce2002types}. Another example is the type representing "Either left or right: \(Either \;a\;b\)", that can be defined with its constructors as \verb!data Either a b = Left a | Right b!. "Either" is a type constructor with the kind "Type -> Type -> Type", meaning it takes two concrete types and returns a concrete type. 

Higher-kinded types are types where there are parenthesis in a kind expression, not including the implicit ones implying right associativity: "\verb|Type -> Type -> Type|" is implicitly "\verb|Type -> (Type -> Type)|"

We can avoid thinking about kinds by enforcing that a type constructor is always given the correct number of arguments

Supporting tagged unions and tuples in the SFL type system would massively increase the ease of writing complex programs. It would also allow for complex data structures such as trees and lists. 
Type names, as well as constructor names, start with uppercase letters in Haskell. This allows them to be easily differentiated from type variables, as well as regular variables. 
In the below definition of the SFL type system, we define \(\Alpha\) as the set of valid type names starting with uppercase letters defined by \(Id\), and \(\alpha\) as the set of valid type variable names defined by \(id\). 

\begin{syntax}[Types in SFL]
(Inbuilts): \(B::=Int\mid Bool\)\newline
(Monotype): \(\tau, \sigma ::= \alpha \mid B \mid \tau \rightarrow \sigma \mid (\tau, \sigma) \mid \Alpha \;T, U,...\)\newline
(Alias): \(A ::= Id = T\)\newline
(Type): \(T, U ::= \alpha \mid B \mid T \rightarrow U \mid (T, U) \mid \forall a. T \mid \Alpha \;T, U,...\)
\end{syntax}
Note that our type constructor application definition above is more permissive than is correct, as it does not enforce correct arity. This can be handled by the parser maintaining the context of the arity of each type. It can then be double checked for debugging purposes via an assertion in the type checker. 

\subsection{Values}


\subsection{Match}
To support different execution based on a condition, we must have some structure that can differentiate between values. 

\subsection{Syntax Sugar}

% -----------------------------------------------------------------------------

\chapter{Implementation}
\label{chap:execution}

\section{The Abstract Syntax Tree}
The tree structure of SFL requires the following different types of tree nodes:
\begin{itemize}
    \item Identifier
    \item Literal
    \item Pair
    \item Application
    \item Abstraction
    \item Case
    \item Assignment
    \item Module
\end{itemize}
Initially, the approach taken when implementing this tree structure was to have each node "owning" its child nodes (see \ref{bg:rust}). However, it will be necessary frequently to be able to find nodes based on certain conditions (for example, the condition that this node is a valid redex) and then provide a value that represents the location of this node within the tree. Even if each of the tree nodes had a unique ID, locating a node from this value representing its location will require some sort of tree search.

Rather than this solution, which would have a non-constant node lookup time, a secondary structure can be used to store the tree nodes with constant time lookup, and then each node can store a value enabling constant time lookup of its children within this structure. In the implementation, these types were labelled as AST and ASTNode, where AST was an array of ASTNodes, and each ASTNode stored their children's indices in this array. The position in the array of an ASTNode will be referred to as its index.

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Rust]
struct AST {
    vec: Vec<ASTNode>,
    root: usize,
}

enum ASTNodeType {
    Identifier,
    Literal,
    Pair,
    Application,
    Assignment,
    Abstraction,
    Module,
} 

struct ASTNodeSyntaxInfo {...}

struct ASTNode {
    t: ASTNodeType,
    token: Option<Token>,
    children: Vec<usize>,
    line: usize,
    col: usize,
    type_assignment: Option<Type>,
    additional_syntax_information: ASTNodeSyntaxInfo
}
    \end{lstlisting}
    \end{tabular}
    \caption{The Rust code listing for the definition of the AST, with lifetime specifiers, accessibility modifiers, and the syntax information (see \ref{paragraph:to_string}) removed for conciseness.}
    \label{fig:ast_lst}
\end{figure}

See \ref{fig:ast_lst} for the code listing for the AST definition. In this implementation, \verb|Vec| was used for the array, as it is growable, resizeable, and facilitates constant-time lookup of its elements. The AST stores and owns all of the nodes, as well as storing the index of the root node rather than requiring it to be at a specific index. 

The node indices in the \verb|children| vector represent different things depending on what kind of node this is. 
\begin{itemize}
    \item If this is an abstraction, the first node represents the variable (or pair of variables) abstracted over, and the second node represents the expression.
    \item If this is an application, the first node is the function, and the second is the argument.
    \item If this is a pair, the first node is the first in the pair, and the second is the second in the pair.
    \item If this is a match expression, the first node represents the matched value, then after this it consists of the case followed by the resulting expression. Match expressions will always therefore have an odd number of children.
    \item If this is a module, then each of the children is an assignment
    \item If this is an assignment, then the first child is the variable being assigned to, and the second is the expression.
\end{itemize}

\verb|Literal| and \verb|Identifier| nodes store the tokens that defined them, so the strings can be accessed. \verb|Identifier| nodes used as abstraction arguments. These types can either be specified in the source program, or inferred later. Nodes also store their positions (line and column) in the source program, which can be used for error messages. 

In order to effectively explain the structure of a parsed program going forwards, the following structure will be used to give a written representation of an AST:
\begin{itemize}
    \item Nodes are represented as one line each, where, with the name of the node type, followed by its value for \verb|Literal|s and \verb|Identifier|s.
    \item The children of a node are all of the nodes with an indentation level one deeper than the node in question listed directly below it, until a shallower or equal depth node is listed. 
\end{itemize}
\filbreak\noindent
For instance, 
\begin{lstlisting}
main = (\x.1) 2
\end{lstlisting}
would be represented as:
\begin{lstlisting}
Module:
  Assignment:
    Identifier: x
    Application:
      Abstraction:
        Identifier: x
        Literal: 1
      Literal: 2
\end{lstlisting}

\subsection{Methods on the AST}
Below are a selection of the more important or interesting methods implemented on the AST and its nodes.

\paragraph{Adding new nodes} We will frequently want to add new nodes to the tree. Where they are inserted is not important, so the tree will add them to the end, and return their index. These methods are needed extensively for the parser.

\paragraph{Getting children of nodes} As the interpretation of the \verb|children| array for each node changes depending on what type of node it is, a series of getters are implemented, such as "\verb|get_func|" to get the function of an application. These methods are needed extensively for the type checker, and the redex finding system. 

\paragraph{Substitute variable} Substitutes all instances of a variable in an expression with a given expression. This is needed for applying abstractions. For instance, the process of reducing \verb|(\x.x) 1|, is:
\begin{itemize}
    \item Get the name of the variable abstracted over: \verb|x|.
    \item Replace all instances of x in the abstraction expression with the left hand side of the application: \verb|1|.
    \item Replace all references to the abstraction with references to the abstractions expression. 
\end{itemize}
Note that this orphans the node for the abstraction, and the node for the abstraction variable \verb|x|. This is hard to rectify as deleting these nodes will shift the rest of the list after it left, which will break many of the references. This is rectified by cloning the AST, as described below.

\paragraph{Clone} The AST, or just a subsection of the AST from a given node, can be cloned by starting from the desired new root, and cloning each nodes children recursively. The new indices of each node may not be the same, as they may be moved in the list, but they will all be in the same place relative to each other. This also removes orphaned nodes, as they will never be cloned as they have no parents. 

\paragraph{To String} \label{paragraph:to_string} Programs can also be effectively transformed back into strings. This requires a few other pieces of information to be associated with some tree nodes, to make the output program as similar to the input program as possible. The more similar the output is to the input, the easier it is to understand. Some examples include:
\begin{itemize}
    \item Whether the application was generated by using the right associative \verb|$| operator in order to avoid parenthesis, for instance \verb|id $ 1 + 1|.
    \item Whether the assignment, where the expression is an abstraction, was generated using the syntax \verb|x = \a.e| or the syntax \verb|x a = e|. These are functionally equivalent, but it is a useful distiction to maintain clarity
\end{itemize}
This is needed for the visualisation of the program state as it changes. 

\section{Types}
Rust allows us to represent our types, as defined in [REFHERE: Type system], quite easily using Enums. Rust's Enums are an example of algebraic data types, and are therefore very useful for defining our own algebraic data type system. See \ref{fig:type_lst} for the listing. 

\begin{figure}[t]
    \centering
    \begin{tabular}{c}
    \begin{lstlisting}[language=Rust]
pub enum Primitive {
    Int64,
    Bool,
}

pub enum Type {
    Unit,
    Primitive(Primitive),
    Function(Box<Type>, Box<Type>),
    TypeVariable(String),
    Forall(String, Box<Type>),
    Product(Box<Type>, Box<Type>),
    Union(String, Vec<Type>),
    Alias(String, Box<Type>),

    Existential(usize),
}
    \end{lstlisting}
    \end{tabular}
    \caption{The Rust code listing for the definition of types}
    \label{fig:type_lst}
\end{figure}

We must use \verb|Box<Type>|, which represents a pointer to a heap allocated object, otherwise it would be impossible to calculate the size of \verb|Type|, as it could be infinitely large with it containing another \verb|Type| recursively. \verb|Box<Type>| however has known size: the size of a pointer in the target architecture. We also define Existential, as an implementation detail needed for the type checker. 

\subsection{Methods on Types}
Below are a selection of the more important or interesting methods implemented on Types.

\paragraph{Substitution of type variables} We may wish to set a type variable to another type. For instance, if given the type expression \(T \; U\) where \(T\) and \(U\) are types, and we know that one of the constructors of \(T\) is of generic type \(\forall a.a \rightarrow T \; a\), the type of the constructor for this type should be \(U \rightarrow T \; U\). We have "instantiated" the type variable \(a\) to be \(U\) by substituting \(a\) with \(U\) throughout the expression, and removing the \(\forall a\). This is required for the type checker. 

\paragraph{To String} We will frequently wish to display types as strings for debugging purposes, as well as to show what types 

\section{The Parser}
The parser needs to consume a program, and return the following things:
\begin{itemize}
    \item The AST
    \item The "Label Table": The types of all labels defined, including both those defined explicitly (assignments) or implicitly (constructors). This is implemented as a struct "\verb|LabelTable| which is a wrapper around a \verb|HashMap<String, Type>| with some useful methods. 
    \item The "Type Table": All type constructors and concrete types defined, stored with their arities. This is implemented as: \verb|HashMap<String, usize>|.
\end{itemize}
For instance, from the program:
\begin{lstlisting}[]
data List a = Cons a (List a) | Nil
double x = x * 2	
main :: List Int
main = Cons (double 1) (Cons (double 2) Nil)
\end{lstlisting}
We should extract the following data:
\begin{itemize}
    \item The AST: {
    \begin{lstlisting}[]
Module:
  Assignent
    Identifier: double
    Abstraction
      Identifer: x
      App
        App
          Identifier: +
          Identifier: x
        Literal: 2
    \end{lstlisting}}
    \item All the known type assignments (excluding inbuilts)
        \begin{itemize}
            \item Cons: \(\forall a. a \Rightarrow List\;a\Rightarrow List\;a\)
            \item Nil: \(\forall a. List\;a\)
            \item main: \(\forall a. List\;a\)
        \end{itemize}
    \item The names of all known types (excluding inbuilts) 
        \begin{itemize}
            \item List, with an arity of 1
        \end{itemize}
\end{itemize}

The parser will also store a set of all bound variables at each location. This will allow us to disqualify some invalid programs while generating the tree, rather than having to traverse it after generation to catch these issues. For instance, we must the following assignments:
\begin{itemize}
    \item x = e where x is already defined, and e is a valid expression, as x is ambiguous during the expression e.
    \item \verb|x = (\x. e)| where e is a valid expression, as x is ambiguous during the expression e. This would be disqualified when attempting to parse the abstraction as x is already bound.  
    \item \verb|x = y| where y is undefined.
\end{itemize}

\subsection{Lexical Analysis}
Lexical analysis is the process splitting a program into its constituent tokens. For instance, the program \verb|main = (\x.x) 1| is the following stream of tokens: \[[Id: main, Assignment, LeftParen, Backslash, Id: x, Dot, Id: x, RightParen, Literal: 1]\]
See \ref{appx:tokens} for the code listing of the definition of the tokens output by the lexical analysis.

\subsection{Expression Parsing}
Expressions are parsed using recursive descent parsing. Some of the techniques used for this part of the parser were inspired by the discussion of top down parsing in \cite{dragon_book}. 

\subsection{Type Assignment and Definition Parsing}
We must also be able to parse type assignments (\verb|x :: Int|) and type definitions (\verb|type a |. Type assignments consist of the label being assigned, followed by the type expression. 

\subsubsection{Parsing Type Expressions}

\subsubsection{Parsing Data Declaration} 
As discussed in [REFHERE: Language Design], we want to be able to define and parse \verb|data| declarations. A \verb|data| deceleration consists of: 
\begin{itemize}
    \item The \verb|data| keyword
    \item The name of the type (uppercase ID)
    \item The Assignment Operator (=)
    \item A set of constructors separated by \(\mid\). Constructor definitions consist of the following. 
        \begin{itemize}
            \item The name of the constructor
            \item Zero or more type expressions, representing what types the constructor can be applied to.
        \end{itemize}
\end{itemize}

An example definition is: "\verb!data Either a b = Left a | Right b!". The information that should be extracted from here is:
\begin{itemize}
    \item "\verb|Either|" is a type constructor with a kind of \(*\rightarrow* \rightarrow *\). As we have no higher kinded types, this can simply be stored as a number representing the arity of the type constructor. In this case, the arity is 2.
    \item The constructors and their types are:
    \begin{itemize}
        \item "\verb|Left|": \(\forall a\;b.a\rightarrow Either\;a\;b\)
        \item "\verb|Nil|": \(\forall a\;b.b\rightarrow Either\;a\;b\)
    \end{itemize}
\end{itemize}
We must store the type name and its arity in the "Type Table", and all constructors in the "Label Table". 

In order to parse the type constructor definition, we continually expect a lowercase identifier token until we reach the assignment operator. The lowercase identifiers declared are passed to the functions that parse constructors so that we can enforce that all of the type variables used in the constructor parsing are "in scope". We also do this to make sure that the variables are correct, and in the correct order in the constructor definitions. 

Parsing constructors is not complex, as we have already implemented the mechanism for parsing type expressions. We simply keep parsing expressions until either the constructor separator ($\mid$) or a newline is reached. When parsing the type expression, we expect only valid concrete types in the Type Table, or valid in scope type variables from the type constructor definition. 

\subsection{Module Parsing}
% -----------------------------------------------------------------------------

\chapter{Critical Evaluation}
\label{chap:evaluation}

% -----------------------------------------------------------------------------

\chapter{Conclusion}
\label{chap:conclusion}

% =============================================================================

% Finally, after the main matter, the back matter is specified.  This is
% typically populated with just the bibliography.  LaTeX deals with these
% in one of two ways, namely
%
% - inline, which roughly means the author specifies entries using the 
%   \bibitem macro and typesets them manually, or
% - using BiBTeX, which means entries are contained in a separate file
%   (which is essentially a databased) then inported; this is the 
%   approach used below, with the databased being dissertation.bib.
%
% Either way, the each entry has a key (or identifier) which can be used
% in the main matter to cite it, e.g., \cite{X}, \cite[Chapter 2}{Y}.
%
% We would recommend using BiBTeX, since it guarantees a consistent referencing style 
% and since many sites (such as dblp) provide references in BiBTeX format. 
% However, note that by default, BiBTeX will ignore capital letters in article titles 
% to ensure consistency of style. This can lead to e.g. "NP-completeness" becoming
% "np-completeness". To avoid this, make sure any capital letters you want to preserve
% are enclosed in braces in the .bib, e.g. "{NP}-completeness".

\backmatter

\bibliography{dissertation}

% -----------------------------------------------------------------------------

% The dissertation concludes with a set of (optional) appendicies; these are 
% the same as chapters in a sense, but once signaled as being appendicies via
% the associated macro, LaTeX manages them appropriatly.

\appendix

\chapter{Appendix A: AI Usage}
\label{appx:ai_prompt}

I did not directly prompt any Large Language Models, or any other AI model, to assist with the writing of my dissertation or implementation. However, as listed in the Supporting Technologies list, I used GitHub Copilot to help with writing some tests for the parser and type checker. I used it via the VS Code extension, which uses the context of your file, to provide advanced AI autocompletion.

% =============================================================================

\chapter{Appendix B: Tokens for Lexical Analysis}
\label{appx:tokens}
Below is the code for how tokens outputted by lexical analysis are defined. 
\begin{lstlisting}
enum TokenType {
    EOF,
    Newline,

    Id,
    UppercaseId,

    If,
    Then,
    Else,

    Match,
    LBrace,
    RBrace,

    IntLit,
    FloatLit,
    StringLit,
    CharLit,
    BoolLit,

    DoubleColon,
    RArrow,
    Forall,
    KWType,
    KWData,

    LParen,
    RParen,

    Lambda,

    Dollar,
    Dot,
    Comma,
    Bar,

    Assignment,
}

struct Token {
    tt: TokenType,
    value: String,
}
\end{lstlisting}

\end{document}
